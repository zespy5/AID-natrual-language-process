{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef7f559",
   "metadata": {},
   "source": [
    "# 임의의 입력 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9f551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80553f",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fd5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 벡터의 차원 : 5, 문장의 길이 : 4\n",
    "# 4번의 timesteps가 존재하고 각 시점마다 5차원의 단어벡터\n",
    "\n",
    "train_X = [[[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]]\n",
    "\n",
    "# 2D 텐서 -> 3D 텐서\n",
    "train_X = np.array(train_X, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a274863f",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695c3954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16dc332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 3)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(3, input_shape = (2, 10)))    # model.add(SimpleRNN(3, input_length = 2, input_dim = 10))와 동일\n",
    "\n",
    "# batch size를 미리 정의하는 경우\n",
    "# model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc14f8",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd127ac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[[ 0.5390469   0.2607186   0.99959356]\n",
      "  [ 0.99135005 -0.8097063   0.99538106]\n",
      "  [ 0.9851441  -0.34994292  0.9947161 ]\n",
      "  [ 0.9347638  -0.7116422   0.9995795 ]]], shape: (1, 4, 3)\n",
      "last hidden state : [[ 0.9347638 -0.7116422  0.9995795]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "rnn = SimpleRNN(3, return_sequences=True, return_state=True)\n",
    "hidden_states, last_state = rnn(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b54a68",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec7418",
   "metadata": {},
   "source": [
    "# RNN으로 텍스트 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc0027",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b805a9",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0acc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93796444",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
    "그의 말이 법이다\\n\n",
    "가는 말이 고와야 오는 말이 곱다\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c37527ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 12\n",
      "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print('단어 집합의 크기 : %d' % vocab_size)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e884415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n",
      "[[ 0  0  0  0  2  3]\n",
      " [ 0  0  0  2  3  1]\n",
      " [ 0  0  2  3  1  4]\n",
      " [ 0  2  3  1  4  5]\n",
      " [ 0  0  0  0  6  1]\n",
      " [ 0  0  0  6  1  7]\n",
      " [ 0  0  0  0  8  1]\n",
      " [ 0  0  0  8  1  9]\n",
      " [ 0  0  8  1  9 10]\n",
      " [ 0  8  1  9 10  1]\n",
      " [ 8  1  9 10  1 11]]\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 생성\n",
    "\n",
    "sequences = list()\n",
    "for line in text.split('\\n'): # 줄바꿈 문자를 기준으로 문장 토큰화\n",
    "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "        \n",
    "print(sequences)\n",
    "\n",
    "# 전체 샘플의 길이를 가장 긴 샘플의 길이로 패딩\n",
    "max_len = max(len(l) for l in sequences) \n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca802f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 레이블 분리\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "\n",
    "# 원-핫 인코딩\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea66f6",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64df858",
   "metadata": {},
   "source": [
    "## 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48ca8a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 - 2s - loss: 2.5104 - accuracy: 0.0000e+00 - 2s/epoch - 2s/step\n",
      "Epoch 2/200\n",
      "1/1 - 0s - loss: 2.4980 - accuracy: 0.0000e+00 - 9ms/epoch - 9ms/step\n",
      "Epoch 3/200\n",
      "1/1 - 0s - loss: 2.4860 - accuracy: 0.0000e+00 - 9ms/epoch - 9ms/step\n",
      "Epoch 4/200\n",
      "1/1 - 0s - loss: 2.4742 - accuracy: 0.0000e+00 - 8ms/epoch - 8ms/step\n",
      "Epoch 5/200\n",
      "1/1 - 0s - loss: 2.4624 - accuracy: 0.0000e+00 - 10ms/epoch - 10ms/step\n",
      "Epoch 6/200\n",
      "1/1 - 0s - loss: 2.4507 - accuracy: 0.0909 - 13ms/epoch - 13ms/step\n",
      "Epoch 7/200\n",
      "1/1 - 0s - loss: 2.4390 - accuracy: 0.1818 - 16ms/epoch - 16ms/step\n",
      "Epoch 8/200\n",
      "1/1 - 0s - loss: 2.4270 - accuracy: 0.2727 - 21ms/epoch - 21ms/step\n",
      "Epoch 9/200\n",
      "1/1 - 0s - loss: 2.4148 - accuracy: 0.4545 - 13ms/epoch - 13ms/step\n",
      "Epoch 10/200\n",
      "1/1 - 0s - loss: 2.4023 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
      "Epoch 11/200\n",
      "1/1 - 0s - loss: 2.3893 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
      "Epoch 12/200\n",
      "1/1 - 0s - loss: 2.3759 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
      "Epoch 13/200\n",
      "1/1 - 0s - loss: 2.3619 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
      "Epoch 14/200\n",
      "1/1 - 0s - loss: 2.3472 - accuracy: 0.4545 - 13ms/epoch - 13ms/step\n",
      "Epoch 15/200\n",
      "1/1 - 0s - loss: 2.3319 - accuracy: 0.4545 - 11ms/epoch - 11ms/step\n",
      "Epoch 16/200\n",
      "1/1 - 0s - loss: 2.3157 - accuracy: 0.4545 - 12ms/epoch - 12ms/step\n",
      "Epoch 17/200\n",
      "1/1 - 0s - loss: 2.2988 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
      "Epoch 18/200\n",
      "1/1 - 0s - loss: 2.2810 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
      "Epoch 19/200\n",
      "1/1 - 0s - loss: 2.2623 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
      "Epoch 20/200\n",
      "1/1 - 0s - loss: 2.2427 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
      "Epoch 21/200\n",
      "1/1 - 0s - loss: 2.2222 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
      "Epoch 22/200\n",
      "1/1 - 0s - loss: 2.2010 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
      "Epoch 23/200\n",
      "1/1 - 0s - loss: 2.1790 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
      "Epoch 24/200\n",
      "1/1 - 0s - loss: 2.1564 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
      "Epoch 25/200\n",
      "1/1 - 0s - loss: 2.1333 - accuracy: 0.3636 - 20ms/epoch - 20ms/step\n",
      "Epoch 26/200\n",
      "1/1 - 0s - loss: 2.1100 - accuracy: 0.3636 - 8ms/epoch - 8ms/step\n",
      "Epoch 27/200\n",
      "1/1 - 0s - loss: 2.0867 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
      "Epoch 28/200\n",
      "1/1 - 0s - loss: 2.0636 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
      "Epoch 29/200\n",
      "1/1 - 0s - loss: 2.0412 - accuracy: 0.3636 - 9ms/epoch - 9ms/step\n",
      "Epoch 30/200\n",
      "1/1 - 0s - loss: 2.0196 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
      "Epoch 31/200\n",
      "1/1 - 0s - loss: 1.9992 - accuracy: 0.3636 - 13ms/epoch - 13ms/step\n",
      "Epoch 32/200\n",
      "1/1 - 0s - loss: 1.9801 - accuracy: 0.3636 - 14ms/epoch - 14ms/step\n",
      "Epoch 33/200\n",
      "1/1 - 0s - loss: 1.9627 - accuracy: 0.3636 - 18ms/epoch - 18ms/step\n",
      "Epoch 34/200\n",
      "1/1 - 0s - loss: 1.9467 - accuracy: 0.3636 - 13ms/epoch - 13ms/step\n",
      "Epoch 35/200\n",
      "1/1 - 0s - loss: 1.9323 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
      "Epoch 36/200\n",
      "1/1 - 0s - loss: 1.9190 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
      "Epoch 37/200\n",
      "1/1 - 0s - loss: 1.9066 - accuracy: 0.3636 - 15ms/epoch - 15ms/step\n",
      "Epoch 38/200\n",
      "1/1 - 0s - loss: 1.8947 - accuracy: 0.3636 - 11ms/epoch - 11ms/step\n",
      "Epoch 39/200\n",
      "1/1 - 0s - loss: 1.8828 - accuracy: 0.3636 - 15ms/epoch - 15ms/step\n",
      "Epoch 40/200\n",
      "1/1 - 0s - loss: 1.8707 - accuracy: 0.3636 - 8ms/epoch - 8ms/step\n",
      "Epoch 41/200\n",
      "1/1 - 0s - loss: 1.8580 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
      "Epoch 42/200\n",
      "1/1 - 0s - loss: 1.8446 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
      "Epoch 43/200\n",
      "1/1 - 0s - loss: 1.8305 - accuracy: 0.3636 - 14ms/epoch - 14ms/step\n",
      "Epoch 44/200\n",
      "1/1 - 0s - loss: 1.8157 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
      "Epoch 45/200\n",
      "1/1 - 0s - loss: 1.8003 - accuracy: 0.3636 - 10ms/epoch - 10ms/step\n",
      "Epoch 46/200\n",
      "1/1 - 0s - loss: 1.7844 - accuracy: 0.3636 - 6ms/epoch - 6ms/step\n",
      "Epoch 47/200\n",
      "1/1 - 0s - loss: 1.7681 - accuracy: 0.4545 - 12ms/epoch - 12ms/step\n",
      "Epoch 48/200\n",
      "1/1 - 0s - loss: 1.7517 - accuracy: 0.4545 - 11ms/epoch - 11ms/step\n",
      "Epoch 49/200\n",
      "1/1 - 0s - loss: 1.7350 - accuracy: 0.4545 - 10ms/epoch - 10ms/step\n",
      "Epoch 50/200\n",
      "1/1 - 0s - loss: 1.7183 - accuracy: 0.4545 - 12ms/epoch - 12ms/step\n",
      "Epoch 51/200\n",
      "1/1 - 0s - loss: 1.7015 - accuracy: 0.4545 - 13ms/epoch - 13ms/step\n",
      "Epoch 52/200\n",
      "1/1 - 0s - loss: 1.6846 - accuracy: 0.4545 - 12ms/epoch - 12ms/step\n",
      "Epoch 53/200\n",
      "1/1 - 0s - loss: 1.6676 - accuracy: 0.4545 - 10ms/epoch - 10ms/step\n",
      "Epoch 54/200\n",
      "1/1 - 0s - loss: 1.6503 - accuracy: 0.4545 - 11ms/epoch - 11ms/step\n",
      "Epoch 55/200\n",
      "1/1 - 0s - loss: 1.6328 - accuracy: 0.4545 - 11ms/epoch - 11ms/step\n",
      "Epoch 56/200\n",
      "1/1 - 0s - loss: 1.6150 - accuracy: 0.4545 - 9ms/epoch - 9ms/step\n",
      "Epoch 57/200\n",
      "1/1 - 0s - loss: 1.5969 - accuracy: 0.4545 - 36ms/epoch - 36ms/step\n",
      "Epoch 58/200\n",
      "1/1 - 0s - loss: 1.5784 - accuracy: 0.4545 - 11ms/epoch - 11ms/step\n",
      "Epoch 59/200\n",
      "1/1 - 0s - loss: 1.5597 - accuracy: 0.4545 - 11ms/epoch - 11ms/step\n",
      "Epoch 60/200\n",
      "1/1 - 0s - loss: 1.5406 - accuracy: 0.4545 - 13ms/epoch - 13ms/step\n",
      "Epoch 61/200\n",
      "1/1 - 0s - loss: 1.5212 - accuracy: 0.4545 - 13ms/epoch - 13ms/step\n",
      "Epoch 62/200\n",
      "1/1 - 0s - loss: 1.5016 - accuracy: 0.4545 - 11ms/epoch - 11ms/step\n",
      "Epoch 63/200\n",
      "1/1 - 0s - loss: 1.4818 - accuracy: 0.5455 - 10ms/epoch - 10ms/step\n",
      "Epoch 64/200\n",
      "1/1 - 0s - loss: 1.4619 - accuracy: 0.5455 - 8ms/epoch - 8ms/step\n",
      "Epoch 65/200\n",
      "1/1 - 0s - loss: 1.4418 - accuracy: 0.5455 - 8ms/epoch - 8ms/step\n",
      "Epoch 66/200\n",
      "1/1 - 0s - loss: 1.4217 - accuracy: 0.5455 - 7ms/epoch - 7ms/step\n",
      "Epoch 67/200\n",
      "1/1 - 0s - loss: 1.4016 - accuracy: 0.5455 - 9ms/epoch - 9ms/step\n",
      "Epoch 68/200\n",
      "1/1 - 0s - loss: 1.3815 - accuracy: 0.5455 - 9ms/epoch - 9ms/step\n",
      "Epoch 69/200\n",
      "1/1 - 0s - loss: 1.3614 - accuracy: 0.5455 - 10ms/epoch - 10ms/step\n",
      "Epoch 70/200\n",
      "1/1 - 0s - loss: 1.3414 - accuracy: 0.6364 - 11ms/epoch - 11ms/step\n",
      "Epoch 71/200\n",
      "1/1 - 0s - loss: 1.3214 - accuracy: 0.6364 - 8ms/epoch - 8ms/step\n",
      "Epoch 72/200\n",
      "1/1 - 0s - loss: 1.3014 - accuracy: 0.6364 - 7ms/epoch - 7ms/step\n",
      "Epoch 73/200\n",
      "1/1 - 0s - loss: 1.2814 - accuracy: 0.6364 - 8ms/epoch - 8ms/step\n",
      "Epoch 74/200\n",
      "1/1 - 0s - loss: 1.2615 - accuracy: 0.6364 - 10ms/epoch - 10ms/step\n",
      "Epoch 75/200\n",
      "1/1 - 0s - loss: 1.2417 - accuracy: 0.6364 - 13ms/epoch - 13ms/step\n",
      "Epoch 76/200\n",
      "1/1 - 0s - loss: 1.2218 - accuracy: 0.6364 - 9ms/epoch - 9ms/step\n",
      "Epoch 77/200\n",
      "1/1 - 0s - loss: 1.2020 - accuracy: 0.6364 - 10ms/epoch - 10ms/step\n",
      "Epoch 78/200\n",
      "1/1 - 0s - loss: 1.1823 - accuracy: 0.6364 - 11ms/epoch - 11ms/step\n",
      "Epoch 79/200\n",
      "1/1 - 0s - loss: 1.1626 - accuracy: 0.6364 - 9ms/epoch - 9ms/step\n",
      "Epoch 80/200\n",
      "1/1 - 0s - loss: 1.1429 - accuracy: 0.6364 - 9ms/epoch - 9ms/step\n",
      "Epoch 81/200\n",
      "1/1 - 0s - loss: 1.1233 - accuracy: 0.6364 - 8ms/epoch - 8ms/step\n",
      "Epoch 82/200\n",
      "1/1 - 0s - loss: 1.1038 - accuracy: 0.6364 - 7ms/epoch - 7ms/step\n",
      "Epoch 83/200\n",
      "1/1 - 0s - loss: 1.0843 - accuracy: 0.6364 - 9ms/epoch - 9ms/step\n",
      "Epoch 84/200\n",
      "1/1 - 0s - loss: 1.0649 - accuracy: 0.6364 - 7ms/epoch - 7ms/step\n",
      "Epoch 85/200\n",
      "1/1 - 0s - loss: 1.0456 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
      "Epoch 86/200\n",
      "1/1 - 0s - loss: 1.0264 - accuracy: 0.7273 - 12ms/epoch - 12ms/step\n",
      "Epoch 87/200\n",
      "1/1 - 0s - loss: 1.0074 - accuracy: 0.7273 - 11ms/epoch - 11ms/step\n",
      "Epoch 88/200\n",
      "1/1 - 0s - loss: 0.9884 - accuracy: 0.7273 - 12ms/epoch - 12ms/step\n",
      "Epoch 89/200\n",
      "1/1 - 0s - loss: 0.9696 - accuracy: 0.7273 - 7ms/epoch - 7ms/step\n",
      "Epoch 90/200\n",
      "1/1 - 0s - loss: 0.9510 - accuracy: 0.7273 - 7ms/epoch - 7ms/step\n",
      "Epoch 91/200\n",
      "1/1 - 0s - loss: 0.9326 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
      "Epoch 92/200\n",
      "1/1 - 0s - loss: 0.9143 - accuracy: 0.7273 - 10ms/epoch - 10ms/step\n",
      "Epoch 93/200\n",
      "1/1 - 0s - loss: 0.8963 - accuracy: 0.7273 - 9ms/epoch - 9ms/step\n",
      "Epoch 94/200\n",
      "1/1 - 0s - loss: 0.8784 - accuracy: 0.7273 - 7ms/epoch - 7ms/step\n",
      "Epoch 95/200\n",
      "1/1 - 0s - loss: 0.8609 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
      "Epoch 96/200\n",
      "1/1 - 0s - loss: 0.8435 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
      "Epoch 97/200\n",
      "1/1 - 0s - loss: 0.8265 - accuracy: 0.8182 - 10ms/epoch - 10ms/step\n",
      "Epoch 98/200\n",
      "1/1 - 0s - loss: 0.8096 - accuracy: 0.8182 - 8ms/epoch - 8ms/step\n",
      "Epoch 99/200\n",
      "1/1 - 0s - loss: 0.7931 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
      "Epoch 100/200\n",
      "1/1 - 0s - loss: 0.7768 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
      "Epoch 101/200\n",
      "1/1 - 0s - loss: 0.7608 - accuracy: 0.8182 - 10ms/epoch - 10ms/step\n",
      "Epoch 102/200\n",
      "1/1 - 0s - loss: 0.7451 - accuracy: 0.8182 - 9ms/epoch - 9ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/200\n",
      "1/1 - 0s - loss: 0.7297 - accuracy: 0.8182 - 10ms/epoch - 10ms/step\n",
      "Epoch 104/200\n",
      "1/1 - 0s - loss: 0.7145 - accuracy: 0.8182 - 8ms/epoch - 8ms/step\n",
      "Epoch 105/200\n",
      "1/1 - 0s - loss: 0.6996 - accuracy: 0.8182 - 7ms/epoch - 7ms/step\n",
      "Epoch 106/200\n",
      "1/1 - 0s - loss: 0.6850 - accuracy: 0.8182 - 8ms/epoch - 8ms/step\n",
      "Epoch 107/200\n",
      "1/1 - 0s - loss: 0.6707 - accuracy: 0.8182 - 9ms/epoch - 9ms/step\n",
      "Epoch 108/200\n",
      "1/1 - 0s - loss: 0.6566 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
      "Epoch 109/200\n",
      "1/1 - 0s - loss: 0.6428 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
      "Epoch 110/200\n",
      "1/1 - 0s - loss: 0.6292 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
      "Epoch 111/200\n",
      "1/1 - 0s - loss: 0.6159 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
      "Epoch 112/200\n",
      "1/1 - 0s - loss: 0.6029 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
      "Epoch 113/200\n",
      "1/1 - 0s - loss: 0.5901 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
      "Epoch 114/200\n",
      "1/1 - 0s - loss: 0.5775 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
      "Epoch 115/200\n",
      "1/1 - 0s - loss: 0.5652 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
      "Epoch 116/200\n",
      "1/1 - 0s - loss: 0.5531 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
      "Epoch 117/200\n",
      "1/1 - 0s - loss: 0.5413 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
      "Epoch 118/200\n",
      "1/1 - 0s - loss: 0.5297 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
      "Epoch 119/200\n",
      "1/1 - 0s - loss: 0.5183 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
      "Epoch 120/200\n",
      "1/1 - 0s - loss: 0.5071 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
      "Epoch 121/200\n",
      "1/1 - 0s - loss: 0.4962 - accuracy: 0.9091 - 9ms/epoch - 9ms/step\n",
      "Epoch 122/200\n",
      "1/1 - 0s - loss: 0.4855 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
      "Epoch 123/200\n",
      "1/1 - 0s - loss: 0.4750 - accuracy: 0.9091 - 9ms/epoch - 9ms/step\n",
      "Epoch 124/200\n",
      "1/1 - 0s - loss: 0.4647 - accuracy: 0.9091 - 12ms/epoch - 12ms/step\n",
      "Epoch 125/200\n",
      "1/1 - 0s - loss: 0.4547 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
      "Epoch 126/200\n",
      "1/1 - 0s - loss: 0.4448 - accuracy: 0.9091 - 13ms/epoch - 13ms/step\n",
      "Epoch 127/200\n",
      "1/1 - 0s - loss: 0.4352 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
      "Epoch 128/200\n",
      "1/1 - 0s - loss: 0.4258 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
      "Epoch 129/200\n",
      "1/1 - 0s - loss: 0.4166 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
      "Epoch 130/200\n",
      "1/1 - 0s - loss: 0.4076 - accuracy: 0.9091 - 9ms/epoch - 9ms/step\n",
      "Epoch 131/200\n",
      "1/1 - 0s - loss: 0.3988 - accuracy: 0.9091 - 9ms/epoch - 9ms/step\n",
      "Epoch 132/200\n",
      "1/1 - 0s - loss: 0.3902 - accuracy: 0.9091 - 10ms/epoch - 10ms/step\n",
      "Epoch 133/200\n",
      "1/1 - 0s - loss: 0.3818 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
      "Epoch 134/200\n",
      "1/1 - 0s - loss: 0.3735 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
      "Epoch 135/200\n",
      "1/1 - 0s - loss: 0.3655 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
      "Epoch 136/200\n",
      "1/1 - 0s - loss: 0.3577 - accuracy: 0.9091 - 9ms/epoch - 9ms/step\n",
      "Epoch 137/200\n",
      "1/1 - 0s - loss: 0.3500 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
      "Epoch 138/200\n",
      "1/1 - 0s - loss: 0.3425 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
      "Epoch 139/200\n",
      "1/1 - 0s - loss: 0.3353 - accuracy: 0.9091 - 6ms/epoch - 6ms/step\n",
      "Epoch 140/200\n",
      "1/1 - 0s - loss: 0.3281 - accuracy: 0.9091 - 9ms/epoch - 9ms/step\n",
      "Epoch 141/200\n",
      "1/1 - 0s - loss: 0.3212 - accuracy: 0.9091 - 9ms/epoch - 9ms/step\n",
      "Epoch 142/200\n",
      "1/1 - 0s - loss: 0.3144 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
      "Epoch 143/200\n",
      "1/1 - 0s - loss: 0.3078 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
      "Epoch 144/200\n",
      "1/1 - 0s - loss: 0.3013 - accuracy: 0.9091 - 9ms/epoch - 9ms/step\n",
      "Epoch 145/200\n",
      "1/1 - 0s - loss: 0.2950 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
      "Epoch 146/200\n",
      "1/1 - 0s - loss: 0.2889 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
      "Epoch 147/200\n",
      "1/1 - 0s - loss: 0.2829 - accuracy: 0.9091 - 6ms/epoch - 6ms/step\n",
      "Epoch 148/200\n",
      "1/1 - 0s - loss: 0.2770 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
      "Epoch 149/200\n",
      "1/1 - 0s - loss: 0.2713 - accuracy: 0.9091 - 7ms/epoch - 7ms/step\n",
      "Epoch 150/200\n",
      "1/1 - 0s - loss: 0.2658 - accuracy: 0.9091 - 6ms/epoch - 6ms/step\n",
      "Epoch 151/200\n",
      "1/1 - 0s - loss: 0.2603 - accuracy: 0.9091 - 9ms/epoch - 9ms/step\n",
      "Epoch 152/200\n",
      "1/1 - 0s - loss: 0.2550 - accuracy: 0.9091 - 9ms/epoch - 9ms/step\n",
      "Epoch 153/200\n",
      "1/1 - 0s - loss: 0.2499 - accuracy: 0.9091 - 8ms/epoch - 8ms/step\n",
      "Epoch 154/200\n",
      "1/1 - 0s - loss: 0.2448 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 155/200\n",
      "1/1 - 0s - loss: 0.2399 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 156/200\n",
      "1/1 - 0s - loss: 0.2351 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 157/200\n",
      "1/1 - 0s - loss: 0.2304 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 158/200\n",
      "1/1 - 0s - loss: 0.2259 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 159/200\n",
      "1/1 - 0s - loss: 0.2214 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 160/200\n",
      "1/1 - 0s - loss: 0.2171 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 161/200\n",
      "1/1 - 0s - loss: 0.2128 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 162/200\n",
      "1/1 - 0s - loss: 0.2087 - accuracy: 1.0000 - 5ms/epoch - 5ms/step\n",
      "Epoch 163/200\n",
      "1/1 - 0s - loss: 0.2046 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 164/200\n",
      "1/1 - 0s - loss: 0.2007 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 165/200\n",
      "1/1 - 0s - loss: 0.1968 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 166/200\n",
      "1/1 - 0s - loss: 0.1931 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 167/200\n",
      "1/1 - 0s - loss: 0.1894 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 168/200\n",
      "1/1 - 0s - loss: 0.1858 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 169/200\n",
      "1/1 - 0s - loss: 0.1823 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 170/200\n",
      "1/1 - 0s - loss: 0.1789 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 171/200\n",
      "1/1 - 0s - loss: 0.1756 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 172/200\n",
      "1/1 - 0s - loss: 0.1723 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 173/200\n",
      "1/1 - 0s - loss: 0.1692 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 174/200\n",
      "1/1 - 0s - loss: 0.1661 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 175/200\n",
      "1/1 - 0s - loss: 0.1630 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 176/200\n",
      "1/1 - 0s - loss: 0.1601 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 177/200\n",
      "1/1 - 0s - loss: 0.1572 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 178/200\n",
      "1/1 - 0s - loss: 0.1544 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 179/200\n",
      "1/1 - 0s - loss: 0.1516 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 180/200\n",
      "1/1 - 0s - loss: 0.1489 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 181/200\n",
      "1/1 - 0s - loss: 0.1463 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 182/200\n",
      "1/1 - 0s - loss: 0.1437 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 183/200\n",
      "1/1 - 0s - loss: 0.1412 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 184/200\n",
      "1/1 - 0s - loss: 0.1387 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 185/200\n",
      "1/1 - 0s - loss: 0.1363 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 186/200\n",
      "1/1 - 0s - loss: 0.1339 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 187/200\n",
      "1/1 - 0s - loss: 0.1316 - accuracy: 1.0000 - 5ms/epoch - 5ms/step\n",
      "Epoch 188/200\n",
      "1/1 - 0s - loss: 0.1294 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 189/200\n",
      "1/1 - 0s - loss: 0.1272 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 190/200\n",
      "1/1 - 0s - loss: 0.1250 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 191/200\n",
      "1/1 - 0s - loss: 0.1229 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 192/200\n",
      "1/1 - 0s - loss: 0.1209 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 193/200\n",
      "1/1 - 0s - loss: 0.1189 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 194/200\n",
      "1/1 - 0s - loss: 0.1169 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 195/200\n",
      "1/1 - 0s - loss: 0.1150 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 196/200\n",
      "1/1 - 0s - loss: 0.1131 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 197/200\n",
      "1/1 - 0s - loss: 0.1112 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 198/200\n",
      "1/1 - 0s - loss: 0.1094 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 199/200\n",
      "1/1 - 0s - loss: 0.1077 - accuracy: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 200/200\n",
      "1/1 - 0s - loss: 0.1059 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25446d03a60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
    "\n",
    "embedding_dim = 10\n",
    "hidden_units = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(SimpleRNN(hidden_units))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58564b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, tokenizer, current_word, n):\n",
    "    init_word = current_word\n",
    "    sentence = ''\n",
    "\n",
    "    # n번 반복\n",
    "    for _ in range(n):\n",
    "        # 현재 단어에 대한 정수 인코딩과 패딩\n",
    "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
    "        # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
    "        result = model.predict(encoded, verbose=0)\n",
    "        result = np.argmax(result, axis=1)\n",
    "\n",
    "        for word, index in tokenizer.word_index.items(): \n",
    "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\n",
    "            if index == result:\n",
    "                break\n",
    "\n",
    "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        current_word = current_word + ' '  + word\n",
    "\n",
    "        # 예측 단어를 문장에 저장\n",
    "        sentence = sentence + ' ' + word\n",
    "\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dce0fff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경마장에 있는 말이 뛰고 있다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, tokenizer, '경마장에', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "132a124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그의 말이 법이다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, tokenizer, '그의', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15d0863a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가는 말이 고와야 오는 말이 곱다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, tokenizer, '가는', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c5f767",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c28de",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d08f94",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a48ef09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[[-0.02149745 -0.11039421  0.00422487]\n",
      "  [ 0.02829784 -0.1395757   0.02499247]\n",
      "  [-0.026241   -0.12349994  0.09546734]\n",
      "  [-0.03320558 -0.17440006  0.14722748]]], shape: (1, 4, 3)\n",
      "last hidden state : [[-0.03320558 -0.17440006  0.14722748]], shape: (1, 3)\n",
      "last cell state : [[-0.04348563 -0.2547642   0.28488302]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "lstm = LSTM(3, return_sequences=True, return_state=True)\n",
    "hidden_states, last_hidden_state, last_cell_state = lstm(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('last hidden state : {}, shape: {}'.format(last_hidden_state, last_hidden_state.shape))\n",
    "print('last cell state : {}, shape: {}'.format(last_cell_state, last_cell_state.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfba3ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['articleID', 'articleWordCount', 'byline', 'documentType', 'headline',\n",
      "       'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
      "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "\n",
    "df = pd.read_csv('ArticlesApril2018.csv')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baf309fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헤드라인의 값들을 리스트로 저장\n",
    "headline = []\n",
    "headline.extend(list(df.headline.values)) \n",
    "\n",
    "# 노이즈 데이터 제거\n",
    "headline = [word for word in headline if word != \"Unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59a71984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구두점 제거, 소문자화\n",
    "\n",
    "def repreprocessing(raw_sentence):\n",
    "    preproceseed_sentence = raw_sentence.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return ''.join(word for word in preproceseed_sentence if word not in punctuation).lower()\n",
    "\n",
    "preprocessed_headline = [repreprocessing(x) for x in headline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a0903af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_headline)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "293384f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[99, 269],\n",
       " [99, 269, 371],\n",
       " [99, 269, 371, 1115],\n",
       " [99, 269, 371, 1115, 582],\n",
       " [99, 269, 371, 1115, 582, 52],\n",
       " [99, 269, 371, 1115, 582, 52, 7],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10, 1116],\n",
       " [100, 3]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = list()\n",
    "\n",
    "for sentence in preprocessed_headline:\n",
    "\n",
    "    # 각 샘플에 대한 정수 인코딩\n",
    "    encoded = tokenizer.texts_to_sequences([sentence])[0] \n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "sequences[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "784771d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수 상위 582번 단어 : offer\n"
     ]
    }
   ],
   "source": [
    "# 정수가 어떤 단어를 의미하는지 알아보는 것\n",
    "\n",
    "index_to_word = {}\n",
    "for key, value in tokenizer.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n",
    "    index_to_word[value] = key\n",
    "    \n",
    "print('빈도수 상위 582번 단어 : {}'.format(index_to_word[582]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72e53932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 샘플의 길이를 가장 긴 샘플의 길이로 패딩\n",
    "max_len = max(len(l) for l in sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "# 레이블 분리\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bcb265",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b17175",
   "metadata": {},
   "source": [
    "## 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd9ba9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "244/244 - 8s - loss: 7.6514 - accuracy: 0.0268 - 8s/epoch - 35ms/step\n",
      "Epoch 2/200\n",
      "244/244 - 7s - loss: 7.1111 - accuracy: 0.0292 - 7s/epoch - 28ms/step\n",
      "Epoch 3/200\n",
      "244/244 - 7s - loss: 6.9748 - accuracy: 0.0358 - 7s/epoch - 29ms/step\n",
      "Epoch 4/200\n",
      "244/244 - 6s - loss: 6.8481 - accuracy: 0.0450 - 6s/epoch - 25ms/step\n",
      "Epoch 5/200\n",
      "244/244 - 6s - loss: 6.6959 - accuracy: 0.0455 - 6s/epoch - 23ms/step\n",
      "Epoch 6/200\n",
      "244/244 - 6s - loss: 6.5216 - accuracy: 0.0501 - 6s/epoch - 24ms/step\n",
      "Epoch 7/200\n",
      "244/244 - 6s - loss: 6.3307 - accuracy: 0.0541 - 6s/epoch - 23ms/step\n",
      "Epoch 8/200\n",
      "244/244 - 6s - loss: 6.1336 - accuracy: 0.0590 - 6s/epoch - 23ms/step\n",
      "Epoch 9/200\n",
      "244/244 - 6s - loss: 5.9433 - accuracy: 0.0642 - 6s/epoch - 24ms/step\n",
      "Epoch 10/200\n",
      "244/244 - 5s - loss: 5.7591 - accuracy: 0.0665 - 5s/epoch - 21ms/step\n",
      "Epoch 11/200\n",
      "244/244 - 5s - loss: 5.5793 - accuracy: 0.0748 - 5s/epoch - 21ms/step\n",
      "Epoch 12/200\n",
      "244/244 - 5s - loss: 5.4129 - accuracy: 0.0759 - 5s/epoch - 21ms/step\n",
      "Epoch 13/200\n",
      "244/244 - 5s - loss: 5.2559 - accuracy: 0.0809 - 5s/epoch - 22ms/step\n",
      "Epoch 14/200\n",
      "244/244 - 5s - loss: 5.1074 - accuracy: 0.0901 - 5s/epoch - 21ms/step\n",
      "Epoch 15/200\n",
      "244/244 - 5s - loss: 4.9637 - accuracy: 0.0950 - 5s/epoch - 21ms/step\n",
      "Epoch 16/200\n",
      "244/244 - 5s - loss: 4.8280 - accuracy: 0.1053 - 5s/epoch - 21ms/step\n",
      "Epoch 17/200\n",
      "244/244 - 5s - loss: 4.6988 - accuracy: 0.1192 - 5s/epoch - 21ms/step\n",
      "Epoch 18/200\n",
      "244/244 - 5s - loss: 4.5734 - accuracy: 0.1382 - 5s/epoch - 21ms/step\n",
      "Epoch 19/200\n",
      "244/244 - 5s - loss: 4.4520 - accuracy: 0.1497 - 5s/epoch - 21ms/step\n",
      "Epoch 20/200\n",
      "244/244 - 6s - loss: 4.3336 - accuracy: 0.1624 - 6s/epoch - 23ms/step\n",
      "Epoch 21/200\n",
      "244/244 - 6s - loss: 4.2189 - accuracy: 0.1826 - 6s/epoch - 23ms/step\n",
      "Epoch 22/200\n",
      "244/244 - 6s - loss: 4.1086 - accuracy: 0.1979 - 6s/epoch - 24ms/step\n",
      "Epoch 23/200\n",
      "244/244 - 7s - loss: 4.0002 - accuracy: 0.2116 - 7s/epoch - 30ms/step\n",
      "Epoch 24/200\n",
      "244/244 - 7s - loss: 3.8938 - accuracy: 0.2275 - 7s/epoch - 27ms/step\n",
      "Epoch 25/200\n",
      "244/244 - 7s - loss: 3.7965 - accuracy: 0.2464 - 7s/epoch - 30ms/step\n",
      "Epoch 26/200\n",
      "244/244 - 6s - loss: 3.6970 - accuracy: 0.2664 - 6s/epoch - 25ms/step\n",
      "Epoch 27/200\n",
      "244/244 - 6s - loss: 3.6015 - accuracy: 0.2766 - 6s/epoch - 23ms/step\n",
      "Epoch 28/200\n",
      "244/244 - 6s - loss: 3.5093 - accuracy: 0.2982 - 6s/epoch - 24ms/step\n",
      "Epoch 29/200\n",
      "244/244 - 5s - loss: 3.4201 - accuracy: 0.3083 - 5s/epoch - 22ms/step\n",
      "Epoch 30/200\n",
      "244/244 - 5s - loss: 3.3336 - accuracy: 0.3291 - 5s/epoch - 22ms/step\n",
      "Epoch 31/200\n",
      "244/244 - 5s - loss: 3.2504 - accuracy: 0.3446 - 5s/epoch - 22ms/step\n",
      "Epoch 32/200\n",
      "244/244 - 5s - loss: 3.1716 - accuracy: 0.3593 - 5s/epoch - 22ms/step\n",
      "Epoch 33/200\n",
      "244/244 - 5s - loss: 3.0920 - accuracy: 0.3770 - 5s/epoch - 22ms/step\n",
      "Epoch 34/200\n",
      "244/244 - 5s - loss: 3.0180 - accuracy: 0.3851 - 5s/epoch - 22ms/step\n",
      "Epoch 35/200\n",
      "244/244 - 5s - loss: 2.9482 - accuracy: 0.3945 - 5s/epoch - 22ms/step\n",
      "Epoch 36/200\n",
      "244/244 - 5s - loss: 2.8750 - accuracy: 0.4100 - 5s/epoch - 21ms/step\n",
      "Epoch 37/200\n",
      "244/244 - 5s - loss: 2.8068 - accuracy: 0.4234 - 5s/epoch - 22ms/step\n",
      "Epoch 38/200\n",
      "244/244 - 5s - loss: 2.7445 - accuracy: 0.4401 - 5s/epoch - 21ms/step\n",
      "Epoch 39/200\n",
      "244/244 - 5s - loss: 2.6774 - accuracy: 0.4521 - 5s/epoch - 21ms/step\n",
      "Epoch 40/200\n",
      "244/244 - 5s - loss: 2.6186 - accuracy: 0.4652 - 5s/epoch - 22ms/step\n",
      "Epoch 41/200\n",
      "244/244 - 5s - loss: 2.5565 - accuracy: 0.4738 - 5s/epoch - 22ms/step\n",
      "Epoch 42/200\n",
      "244/244 - 5s - loss: 2.5000 - accuracy: 0.4826 - 5s/epoch - 21ms/step\n",
      "Epoch 43/200\n",
      "244/244 - 6s - loss: 2.4452 - accuracy: 0.4929 - 6s/epoch - 23ms/step\n",
      "Epoch 44/200\n",
      "244/244 - 6s - loss: 2.3916 - accuracy: 0.5038 - 6s/epoch - 23ms/step\n",
      "Epoch 45/200\n",
      "244/244 - 6s - loss: 2.3360 - accuracy: 0.5149 - 6s/epoch - 23ms/step\n",
      "Epoch 46/200\n",
      "244/244 - 6s - loss: 2.2853 - accuracy: 0.5301 - 6s/epoch - 25ms/step\n",
      "Epoch 47/200\n",
      "244/244 - 6s - loss: 2.2342 - accuracy: 0.5426 - 6s/epoch - 26ms/step\n",
      "Epoch 48/200\n",
      "244/244 - 6s - loss: 2.1870 - accuracy: 0.5507 - 6s/epoch - 26ms/step\n",
      "Epoch 49/200\n",
      "244/244 - 6s - loss: 2.1389 - accuracy: 0.5562 - 6s/epoch - 26ms/step\n",
      "Epoch 50/200\n",
      "244/244 - 6s - loss: 2.0882 - accuracy: 0.5723 - 6s/epoch - 23ms/step\n",
      "Epoch 51/200\n",
      "244/244 - 5s - loss: 2.0443 - accuracy: 0.5807 - 5s/epoch - 22ms/step\n",
      "Epoch 52/200\n",
      "244/244 - 5s - loss: 2.0017 - accuracy: 0.5877 - 5s/epoch - 22ms/step\n",
      "Epoch 53/200\n",
      "244/244 - 5s - loss: 1.9535 - accuracy: 0.6003 - 5s/epoch - 22ms/step\n",
      "Epoch 54/200\n",
      "244/244 - 5s - loss: 1.9126 - accuracy: 0.6054 - 5s/epoch - 21ms/step\n",
      "Epoch 55/200\n",
      "244/244 - 5s - loss: 1.8713 - accuracy: 0.6203 - 5s/epoch - 22ms/step\n",
      "Epoch 56/200\n",
      "244/244 - 5s - loss: 1.8302 - accuracy: 0.6275 - 5s/epoch - 22ms/step\n",
      "Epoch 57/200\n",
      "244/244 - 5s - loss: 1.7891 - accuracy: 0.6353 - 5s/epoch - 21ms/step\n",
      "Epoch 58/200\n",
      "244/244 - 6s - loss: 1.7523 - accuracy: 0.6431 - 6s/epoch - 23ms/step\n",
      "Epoch 59/200\n",
      "244/244 - 6s - loss: 1.7144 - accuracy: 0.6545 - 6s/epoch - 24ms/step\n",
      "Epoch 60/200\n",
      "244/244 - 6s - loss: 1.6738 - accuracy: 0.6604 - 6s/epoch - 23ms/step\n",
      "Epoch 61/200\n",
      "244/244 - 6s - loss: 1.6383 - accuracy: 0.6690 - 6s/epoch - 25ms/step\n",
      "Epoch 62/200\n",
      "244/244 - 6s - loss: 1.6017 - accuracy: 0.6773 - 6s/epoch - 25ms/step\n",
      "Epoch 63/200\n",
      "244/244 - 6s - loss: 1.5664 - accuracy: 0.6887 - 6s/epoch - 25ms/step\n",
      "Epoch 64/200\n",
      "244/244 - 6s - loss: 1.5308 - accuracy: 0.6909 - 6s/epoch - 24ms/step\n",
      "Epoch 65/200\n",
      "244/244 - 5s - loss: 1.5034 - accuracy: 0.6974 - 5s/epoch - 22ms/step\n",
      "Epoch 66/200\n",
      "244/244 - 5s - loss: 1.4679 - accuracy: 0.7040 - 5s/epoch - 22ms/step\n",
      "Epoch 67/200\n",
      "244/244 - 5s - loss: 1.4329 - accuracy: 0.7140 - 5s/epoch - 22ms/step\n",
      "Epoch 68/200\n",
      "244/244 - 5s - loss: 1.4015 - accuracy: 0.7184 - 5s/epoch - 22ms/step\n",
      "Epoch 69/200\n",
      "244/244 - 5s - loss: 1.3705 - accuracy: 0.7236 - 5s/epoch - 22ms/step\n",
      "Epoch 70/200\n",
      "244/244 - 5s - loss: 1.3434 - accuracy: 0.7343 - 5s/epoch - 22ms/step\n",
      "Epoch 71/200\n",
      "244/244 - 5s - loss: 1.3100 - accuracy: 0.7392 - 5s/epoch - 21ms/step\n",
      "Epoch 72/200\n",
      "244/244 - 5s - loss: 1.2809 - accuracy: 0.7460 - 5s/epoch - 22ms/step\n",
      "Epoch 73/200\n",
      "244/244 - 5s - loss: 1.2507 - accuracy: 0.7521 - 5s/epoch - 22ms/step\n",
      "Epoch 74/200\n",
      "244/244 - 5s - loss: 1.2251 - accuracy: 0.7550 - 5s/epoch - 22ms/step\n",
      "Epoch 75/200\n",
      "244/244 - 5s - loss: 1.2009 - accuracy: 0.7630 - 5s/epoch - 22ms/step\n",
      "Epoch 76/200\n",
      "244/244 - 5s - loss: 1.1767 - accuracy: 0.7698 - 5s/epoch - 21ms/step\n",
      "Epoch 77/200\n",
      "244/244 - 5s - loss: 1.1430 - accuracy: 0.7748 - 5s/epoch - 22ms/step\n",
      "Epoch 78/200\n",
      "244/244 - 5s - loss: 1.1248 - accuracy: 0.7760 - 5s/epoch - 22ms/step\n",
      "Epoch 79/200\n",
      "244/244 - 5s - loss: 1.1003 - accuracy: 0.7832 - 5s/epoch - 22ms/step\n",
      "Epoch 80/200\n",
      "244/244 - 5s - loss: 1.0725 - accuracy: 0.7871 - 5s/epoch - 22ms/step\n",
      "Epoch 81/200\n",
      "244/244 - 5s - loss: 1.0475 - accuracy: 0.7947 - 5s/epoch - 22ms/step\n",
      "Epoch 82/200\n",
      "244/244 - 5s - loss: 1.0228 - accuracy: 0.7965 - 5s/epoch - 22ms/step\n",
      "Epoch 83/200\n",
      "244/244 - 5s - loss: 1.0030 - accuracy: 0.8012 - 5s/epoch - 22ms/step\n",
      "Epoch 84/200\n",
      "244/244 - 5s - loss: 0.9827 - accuracy: 0.8049 - 5s/epoch - 22ms/step\n",
      "Epoch 85/200\n",
      "244/244 - 5s - loss: 0.9570 - accuracy: 0.8107 - 5s/epoch - 22ms/step\n",
      "Epoch 86/200\n",
      "244/244 - 5s - loss: 0.9363 - accuracy: 0.8149 - 5s/epoch - 22ms/step\n",
      "Epoch 87/200\n",
      "244/244 - 6s - loss: 0.9157 - accuracy: 0.8201 - 6s/epoch - 24ms/step\n",
      "Epoch 88/200\n",
      "244/244 - 5s - loss: 0.8963 - accuracy: 0.8225 - 5s/epoch - 22ms/step\n",
      "Epoch 89/200\n",
      "244/244 - 5s - loss: 0.8757 - accuracy: 0.8281 - 5s/epoch - 23ms/step\n",
      "Epoch 90/200\n",
      "244/244 - 5s - loss: 0.8555 - accuracy: 0.8321 - 5s/epoch - 22ms/step\n",
      "Epoch 91/200\n",
      "244/244 - 5s - loss: 0.8375 - accuracy: 0.8358 - 5s/epoch - 22ms/step\n",
      "Epoch 92/200\n",
      "244/244 - 5s - loss: 0.8199 - accuracy: 0.8393 - 5s/epoch - 22ms/step\n",
      "Epoch 93/200\n",
      "244/244 - 5s - loss: 0.7991 - accuracy: 0.8403 - 5s/epoch - 22ms/step\n",
      "Epoch 94/200\n",
      "244/244 - 5s - loss: 0.7814 - accuracy: 0.8451 - 5s/epoch - 22ms/step\n",
      "Epoch 95/200\n",
      "244/244 - 5s - loss: 0.7652 - accuracy: 0.8503 - 5s/epoch - 22ms/step\n",
      "Epoch 96/200\n",
      "244/244 - 5s - loss: 0.7467 - accuracy: 0.8530 - 5s/epoch - 22ms/step\n",
      "Epoch 97/200\n",
      "244/244 - 5s - loss: 0.7333 - accuracy: 0.8554 - 5s/epoch - 22ms/step\n",
      "Epoch 98/200\n",
      "244/244 - 5s - loss: 0.7165 - accuracy: 0.8571 - 5s/epoch - 22ms/step\n",
      "Epoch 99/200\n",
      "244/244 - 5s - loss: 0.7008 - accuracy: 0.8617 - 5s/epoch - 22ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      "244/244 - 5s - loss: 0.6855 - accuracy: 0.8644 - 5s/epoch - 22ms/step\n",
      "Epoch 101/200\n",
      "244/244 - 5s - loss: 0.6703 - accuracy: 0.8703 - 5s/epoch - 22ms/step\n",
      "Epoch 102/200\n",
      "244/244 - 5s - loss: 0.6571 - accuracy: 0.8715 - 5s/epoch - 22ms/step\n",
      "Epoch 103/200\n",
      "244/244 - 5s - loss: 0.6412 - accuracy: 0.8721 - 5s/epoch - 22ms/step\n",
      "Epoch 104/200\n",
      "244/244 - 5s - loss: 0.6272 - accuracy: 0.8748 - 5s/epoch - 22ms/step\n",
      "Epoch 105/200\n",
      "244/244 - 5s - loss: 0.6157 - accuracy: 0.8802 - 5s/epoch - 22ms/step\n",
      "Epoch 106/200\n",
      "244/244 - 5s - loss: 0.6011 - accuracy: 0.8784 - 5s/epoch - 22ms/step\n",
      "Epoch 107/200\n",
      "244/244 - 5s - loss: 0.5882 - accuracy: 0.8826 - 5s/epoch - 22ms/step\n",
      "Epoch 108/200\n",
      "244/244 - 5s - loss: 0.5758 - accuracy: 0.8841 - 5s/epoch - 22ms/step\n",
      "Epoch 109/200\n",
      "244/244 - 6s - loss: 0.5638 - accuracy: 0.8857 - 6s/epoch - 23ms/step\n",
      "Epoch 110/200\n",
      "244/244 - 6s - loss: 0.5539 - accuracy: 0.8868 - 6s/epoch - 23ms/step\n",
      "Epoch 111/200\n",
      "244/244 - 6s - loss: 0.5440 - accuracy: 0.8884 - 6s/epoch - 23ms/step\n",
      "Epoch 112/200\n",
      "244/244 - 6s - loss: 0.5295 - accuracy: 0.8922 - 6s/epoch - 23ms/step\n",
      "Epoch 113/200\n",
      "244/244 - 5s - loss: 0.5205 - accuracy: 0.8947 - 5s/epoch - 22ms/step\n",
      "Epoch 114/200\n",
      "244/244 - 5s - loss: 0.5097 - accuracy: 0.8962 - 5s/epoch - 22ms/step\n",
      "Epoch 115/200\n",
      "244/244 - 5s - loss: 0.5004 - accuracy: 0.8956 - 5s/epoch - 22ms/step\n",
      "Epoch 116/200\n",
      "244/244 - 5s - loss: 0.4903 - accuracy: 0.8977 - 5s/epoch - 22ms/step\n",
      "Epoch 117/200\n",
      "244/244 - 5s - loss: 0.4817 - accuracy: 0.8988 - 5s/epoch - 22ms/step\n",
      "Epoch 118/200\n",
      "244/244 - 5s - loss: 0.4721 - accuracy: 0.8993 - 5s/epoch - 22ms/step\n",
      "Epoch 119/200\n",
      "244/244 - 5s - loss: 0.5083 - accuracy: 0.8944 - 5s/epoch - 22ms/step\n",
      "Epoch 120/200\n",
      "244/244 - 5s - loss: 0.4778 - accuracy: 0.8985 - 5s/epoch - 22ms/step\n",
      "Epoch 121/200\n",
      "244/244 - 5s - loss: 0.4485 - accuracy: 0.9047 - 5s/epoch - 22ms/step\n",
      "Epoch 122/200\n",
      "244/244 - 5s - loss: 0.4368 - accuracy: 0.9048 - 5s/epoch - 22ms/step\n",
      "Epoch 123/200\n",
      "244/244 - 5s - loss: 0.4298 - accuracy: 0.9052 - 5s/epoch - 22ms/step\n",
      "Epoch 124/200\n",
      "244/244 - 5s - loss: 0.4235 - accuracy: 0.9068 - 5s/epoch - 22ms/step\n",
      "Epoch 125/200\n",
      "244/244 - 5s - loss: 0.4161 - accuracy: 0.9076 - 5s/epoch - 22ms/step\n",
      "Epoch 126/200\n",
      "244/244 - 5s - loss: 0.4122 - accuracy: 0.9088 - 5s/epoch - 22ms/step\n",
      "Epoch 127/200\n",
      "244/244 - 5s - loss: 0.4053 - accuracy: 0.9100 - 5s/epoch - 22ms/step\n",
      "Epoch 128/200\n",
      "244/244 - 5s - loss: 0.4002 - accuracy: 0.9098 - 5s/epoch - 22ms/step\n",
      "Epoch 129/200\n",
      "244/244 - 5s - loss: 0.3930 - accuracy: 0.9099 - 5s/epoch - 22ms/step\n",
      "Epoch 130/200\n",
      "244/244 - 5s - loss: 0.3873 - accuracy: 0.9098 - 5s/epoch - 22ms/step\n",
      "Epoch 131/200\n",
      "244/244 - 5s - loss: 0.3823 - accuracy: 0.9121 - 5s/epoch - 22ms/step\n",
      "Epoch 132/200\n",
      "244/244 - 5s - loss: 0.3794 - accuracy: 0.9114 - 5s/epoch - 22ms/step\n",
      "Epoch 133/200\n",
      "244/244 - 5s - loss: 0.3772 - accuracy: 0.9127 - 5s/epoch - 22ms/step\n",
      "Epoch 134/200\n",
      "244/244 - 5s - loss: 0.3696 - accuracy: 0.9126 - 5s/epoch - 22ms/step\n",
      "Epoch 135/200\n",
      "244/244 - 5s - loss: 0.3696 - accuracy: 0.9134 - 5s/epoch - 22ms/step\n",
      "Epoch 136/200\n",
      "244/244 - 5s - loss: 0.3601 - accuracy: 0.9132 - 5s/epoch - 23ms/step\n",
      "Epoch 137/200\n",
      "244/244 - 5s - loss: 0.3534 - accuracy: 0.9143 - 5s/epoch - 22ms/step\n",
      "Epoch 138/200\n",
      "244/244 - 5s - loss: 0.3500 - accuracy: 0.9139 - 5s/epoch - 22ms/step\n",
      "Epoch 139/200\n",
      "244/244 - 5s - loss: 0.3452 - accuracy: 0.9145 - 5s/epoch - 22ms/step\n",
      "Epoch 140/200\n",
      "244/244 - 5s - loss: 0.3440 - accuracy: 0.9148 - 5s/epoch - 22ms/step\n",
      "Epoch 141/200\n",
      "244/244 - 5s - loss: 0.3427 - accuracy: 0.9149 - 5s/epoch - 22ms/step\n",
      "Epoch 142/200\n",
      "244/244 - 5s - loss: 0.3390 - accuracy: 0.9158 - 5s/epoch - 22ms/step\n",
      "Epoch 143/200\n",
      "244/244 - 5s - loss: 0.3325 - accuracy: 0.9153 - 5s/epoch - 22ms/step\n",
      "Epoch 144/200\n",
      "244/244 - 5s - loss: 0.3288 - accuracy: 0.9158 - 5s/epoch - 22ms/step\n",
      "Epoch 145/200\n",
      "244/244 - 5s - loss: 0.3252 - accuracy: 0.9162 - 5s/epoch - 22ms/step\n",
      "Epoch 146/200\n",
      "244/244 - 5s - loss: 0.3236 - accuracy: 0.9155 - 5s/epoch - 22ms/step\n",
      "Epoch 147/200\n",
      "244/244 - 5s - loss: 0.3212 - accuracy: 0.9158 - 5s/epoch - 22ms/step\n",
      "Epoch 148/200\n",
      "244/244 - 5s - loss: 0.3173 - accuracy: 0.9167 - 5s/epoch - 22ms/step\n",
      "Epoch 149/200\n",
      "244/244 - 5s - loss: 0.3151 - accuracy: 0.9166 - 5s/epoch - 22ms/step\n",
      "Epoch 150/200\n",
      "244/244 - 5s - loss: 0.3132 - accuracy: 0.9163 - 5s/epoch - 22ms/step\n",
      "Epoch 151/200\n",
      "244/244 - 5s - loss: 0.3120 - accuracy: 0.9163 - 5s/epoch - 22ms/step\n",
      "Epoch 152/200\n",
      "244/244 - 6s - loss: 0.3366 - accuracy: 0.9121 - 6s/epoch - 23ms/step\n",
      "Epoch 153/200\n",
      "244/244 - 5s - loss: 0.3103 - accuracy: 0.9158 - 5s/epoch - 22ms/step\n",
      "Epoch 154/200\n",
      "244/244 - 5s - loss: 0.3053 - accuracy: 0.9181 - 5s/epoch - 22ms/step\n",
      "Epoch 155/200\n",
      "244/244 - 5s - loss: 0.3022 - accuracy: 0.9164 - 5s/epoch - 22ms/step\n",
      "Epoch 156/200\n",
      "244/244 - 5s - loss: 0.2992 - accuracy: 0.9175 - 5s/epoch - 22ms/step\n",
      "Epoch 157/200\n",
      "244/244 - 5s - loss: 0.2986 - accuracy: 0.9166 - 5s/epoch - 22ms/step\n",
      "Epoch 158/200\n",
      "244/244 - 5s - loss: 0.2957 - accuracy: 0.9152 - 5s/epoch - 22ms/step\n",
      "Epoch 159/200\n",
      "244/244 - 5s - loss: 0.2946 - accuracy: 0.9154 - 5s/epoch - 22ms/step\n",
      "Epoch 160/200\n",
      "244/244 - 6s - loss: 0.2944 - accuracy: 0.9157 - 6s/epoch - 23ms/step\n",
      "Epoch 161/200\n",
      "244/244 - 6s - loss: 0.2941 - accuracy: 0.9176 - 6s/epoch - 26ms/step\n",
      "Epoch 162/200\n",
      "244/244 - 7s - loss: 0.2923 - accuracy: 0.9163 - 7s/epoch - 27ms/step\n",
      "Epoch 163/200\n",
      "244/244 - 7s - loss: 0.3041 - accuracy: 0.9134 - 7s/epoch - 28ms/step\n",
      "Epoch 164/200\n",
      "244/244 - 6s - loss: 0.3184 - accuracy: 0.9122 - 6s/epoch - 25ms/step\n",
      "Epoch 165/200\n",
      "244/244 - 6s - loss: 0.2966 - accuracy: 0.9153 - 6s/epoch - 26ms/step\n",
      "Epoch 166/200\n",
      "244/244 - 6s - loss: 0.2881 - accuracy: 0.9172 - 6s/epoch - 23ms/step\n",
      "Epoch 167/200\n",
      "244/244 - 5s - loss: 0.2842 - accuracy: 0.9180 - 5s/epoch - 22ms/step\n",
      "Epoch 168/200\n",
      "244/244 - 5s - loss: 0.2828 - accuracy: 0.9176 - 5s/epoch - 22ms/step\n",
      "Epoch 169/200\n",
      "244/244 - 5s - loss: 0.2833 - accuracy: 0.9153 - 5s/epoch - 22ms/step\n",
      "Epoch 170/200\n",
      "244/244 - 5s - loss: 0.2811 - accuracy: 0.9158 - 5s/epoch - 22ms/step\n",
      "Epoch 171/200\n",
      "244/244 - 5s - loss: 0.2812 - accuracy: 0.9162 - 5s/epoch - 22ms/step\n",
      "Epoch 172/200\n",
      "244/244 - 5s - loss: 0.2795 - accuracy: 0.9161 - 5s/epoch - 22ms/step\n",
      "Epoch 173/200\n",
      "244/244 - 5s - loss: 0.2797 - accuracy: 0.9146 - 5s/epoch - 22ms/step\n",
      "Epoch 174/200\n",
      "244/244 - 5s - loss: 0.2779 - accuracy: 0.9163 - 5s/epoch - 22ms/step\n",
      "Epoch 175/200\n",
      "244/244 - 5s - loss: 0.2780 - accuracy: 0.9157 - 5s/epoch - 22ms/step\n",
      "Epoch 176/200\n",
      "244/244 - 5s - loss: 0.2770 - accuracy: 0.9157 - 5s/epoch - 22ms/step\n",
      "Epoch 177/200\n",
      "244/244 - 5s - loss: 0.2771 - accuracy: 0.9173 - 5s/epoch - 22ms/step\n",
      "Epoch 178/200\n",
      "244/244 - 6s - loss: 0.2761 - accuracy: 0.9163 - 6s/epoch - 23ms/step\n",
      "Epoch 179/200\n",
      "244/244 - 6s - loss: 0.2761 - accuracy: 0.9172 - 6s/epoch - 26ms/step\n",
      "Epoch 180/200\n",
      "244/244 - 7s - loss: 0.2746 - accuracy: 0.9159 - 7s/epoch - 27ms/step\n",
      "Epoch 181/200\n",
      "244/244 - 6s - loss: 0.2722 - accuracy: 0.9182 - 6s/epoch - 26ms/step\n",
      "Epoch 182/200\n",
      "244/244 - 7s - loss: 0.2995 - accuracy: 0.9099 - 7s/epoch - 29ms/step\n",
      "Epoch 183/200\n",
      "244/244 - 6s - loss: 0.3095 - accuracy: 0.9111 - 6s/epoch - 25ms/step\n",
      "Epoch 184/200\n",
      "244/244 - 6s - loss: 0.2810 - accuracy: 0.9161 - 6s/epoch - 23ms/step\n",
      "Epoch 185/200\n",
      "244/244 - 5s - loss: 0.2726 - accuracy: 0.9163 - 5s/epoch - 22ms/step\n",
      "Epoch 186/200\n",
      "244/244 - 5s - loss: 0.2689 - accuracy: 0.9185 - 5s/epoch - 23ms/step\n",
      "Epoch 187/200\n",
      "244/244 - 5s - loss: 0.2694 - accuracy: 0.9158 - 5s/epoch - 22ms/step\n",
      "Epoch 188/200\n",
      "244/244 - 6s - loss: 0.2684 - accuracy: 0.9161 - 6s/epoch - 23ms/step\n",
      "Epoch 189/200\n",
      "244/244 - 6s - loss: 0.2684 - accuracy: 0.9163 - 6s/epoch - 23ms/step\n",
      "Epoch 190/200\n",
      "244/244 - 5s - loss: 0.2677 - accuracy: 0.9167 - 5s/epoch - 22ms/step\n",
      "Epoch 191/200\n",
      "244/244 - 6s - loss: 0.2680 - accuracy: 0.9182 - 6s/epoch - 23ms/step\n",
      "Epoch 192/200\n",
      "244/244 - 6s - loss: 0.2681 - accuracy: 0.9154 - 6s/epoch - 26ms/step\n",
      "Epoch 193/200\n",
      "244/244 - 6s - loss: 0.2670 - accuracy: 0.9164 - 6s/epoch - 26ms/step\n",
      "Epoch 194/200\n",
      "244/244 - 6s - loss: 0.2667 - accuracy: 0.9167 - 6s/epoch - 24ms/step\n",
      "Epoch 195/200\n",
      "244/244 - 6s - loss: 0.2668 - accuracy: 0.9168 - 6s/epoch - 25ms/step\n",
      "Epoch 196/200\n",
      "244/244 - 6s - loss: 0.2666 - accuracy: 0.9168 - 6s/epoch - 24ms/step\n",
      "Epoch 197/200\n",
      "244/244 - 6s - loss: 0.2670 - accuracy: 0.9152 - 6s/epoch - 26ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/200\n",
      "244/244 - 7s - loss: 0.2655 - accuracy: 0.9163 - 7s/epoch - 28ms/step\n",
      "Epoch 199/200\n",
      "244/244 - 7s - loss: 0.2754 - accuracy: 0.9132 - 7s/epoch - 27ms/step\n",
      "Epoch 200/200\n",
      "244/244 - 6s - loss: 0.2750 - accuracy: 0.9166 - 6s/epoch - 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2544a2821d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "\n",
    "embedding_dim = 10\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a60efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word\n",
    "    sentence = ''\n",
    "\n",
    "    # n번 반복\n",
    "    for _ in range(n):\n",
    "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=max_len-1, padding='pre')\n",
    "\n",
    "        # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
    "        result = model.predict(encoded, verbose=0)\n",
    "        result = np.argmax(result, axis=1)\n",
    "\n",
    "        for word, index in tokenizer.word_index.items(): \n",
    "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "            if index == result:\n",
    "                break\n",
    "\n",
    "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        current_word = current_word + ' '  + word\n",
    "\n",
    "        # 예측 단어를 문장에 저장\n",
    "        sentence = sentence + ' ' + word\n",
    "\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9011cea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i disapprove of school vouchers can i still apply for them\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, tokenizer, 'i', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7baf563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to make facebook more accountable on immigration immigrants risks to\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, tokenizer, 'how', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6329559",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c0952",
   "metadata": {},
   "source": [
    "# 양방향 순환 신경망(Bidirectional Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbf937c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "timesteps = 10\n",
    "input_dim = 5\n",
    "hidden_units = 8\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True), input_shape=(timesteps, input_dim)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646c03e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed2281c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_init = tf.keras.initializers.Constant(value=0.1)\n",
    "b_init = tf.keras.initializers.Constant(value=0)\n",
    "r_init = tf.keras.initializers.Constant(value=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24241e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[0.6303138 0.6303138 0.6303138 0.7038734 0.7038734 0.7038734]], shape: (1, 6)\n",
      "forward state : [[0.6303138 0.6303138 0.6303138]], shape: (1, 3)\n",
      "backward state : [[0.7038734 0.7038734 0.7038734]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "bilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True,\n",
    "                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\n",
    "hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\n",
    "print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d882cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[[0.35906473 0.35906473 0.35906473 0.7038734  0.7038734  0.7038734 ]\n",
      "  [0.55111325 0.55111325 0.55111325 0.58863586 0.58863586 0.58863586]\n",
      "  [0.59115744 0.59115744 0.59115744 0.3951699  0.3951699  0.3951699 ]\n",
      "  [0.6303138  0.6303138  0.6303138  0.21942244 0.21942244 0.21942244]]], shape: (1, 4, 6)\n",
      "forward state : [[0.6303138 0.6303138 0.6303138]], shape: (1, 3)\n",
      "backward state : [[0.7038734 0.7038734 0.7038734]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# return_sequences=True인 경우\n",
    "\n",
    "bilstm = Bidirectional(LSTM(3, return_sequences=True, return_state=True,\n",
    "                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\n",
    "hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\n",
    "print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66fe04b",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c005b37",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2fe0b",
   "metadata": {},
   "source": [
    "# GRU(Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64245199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "hidden_size = 12\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5948fdf6",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bf2d05",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13c582",
   "metadata": {},
   "source": [
    "#  문자 단위 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c7761",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eff38f",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51dd5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 데이터 로드\n",
    "urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename=\"11-0.txt\")\n",
    "\n",
    "f = open('11-0.txt', 'rb')\n",
    "sentences = []\n",
    "for sentence in f: # 데이터로부터 한 줄씩 읽는다.\n",
    "    sentence = sentence.strip() # strip()을 통해 \\r, \\n을 제거한다.\n",
    "    sentence = sentence.lower() # 소문자화.\n",
    "    sentence = sentence.decode('ascii', 'ignore') # \\xe2\\x80\\x99 등과 같은 바이트 열 제거\n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "f.close()\n",
    "\n",
    "total_data = ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea21b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 집합\n",
    "char_vocab = sorted(list(set(total_data)))\n",
    "vocab_size = len(char_vocab)\n",
    "\n",
    "# 정수 부여\n",
    "char_to_index = dict((char, index) for index, char in enumerate(char_vocab))\n",
    "\n",
    "# 정수로부터 문자 리턴\n",
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68af82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 만들기\n",
    "\n",
    "seq_length = 60\n",
    "n_samples = int(np.floor((len(total_data) - 1) / seq_length))\n",
    "\n",
    "\n",
    "train_X = []\n",
    "train_y = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # 0:60 -> 60:120 -> 120:180로 loop를 돌면서 문장 샘플을 1개씩 pick.\n",
    "    X_sample = total_data[i * seq_length: (i + 1) * seq_length]\n",
    "    \n",
    "    X_encoded = [char_to_index[c] for c in X_sample]\n",
    "    train_X.append(X_encoded)\n",
    "\n",
    "    # 오른쪽으로 1칸 쉬프트\n",
    "    y_sample = total_data[i * seq_length + 1: (i + 1) * seq_length + 1]\n",
    "    y_encoded = [char_to_index[c] for c in y_sample]\n",
    "    train_y.append(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36a0a3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54]\n",
      "[33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54, 52]\n"
     ]
    }
   ],
   "source": [
    "print(train_X[1])\n",
    "print(train_y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9308788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X의 크기(shape) : (2658, 60, 56)\n",
      "train_y의 크기(shape) : (2658, 60, 56)\n"
     ]
    }
   ],
   "source": [
    "# 원-핫 인코딩\n",
    "train_X = to_categorical(train_X)\n",
    "train_y = to_categorical(train_y)\n",
    "\n",
    "print('train_X의 크기(shape) : {}'.format(train_X.shape)) # 원-핫 인코딩\n",
    "print('train_y의 크기(shape) : {}'.format(train_y.shape)) # 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d4376",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6700be1a",
   "metadata": {},
   "source": [
    "## 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1f82ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "84/84 - 29s - loss: 3.0745 - accuracy: 0.1799 - 29s/epoch - 349ms/step\n",
      "Epoch 2/80\n",
      "84/84 - 28s - loss: 2.7248 - accuracy: 0.2498 - 28s/epoch - 334ms/step\n",
      "Epoch 3/80\n",
      "84/84 - 25s - loss: 2.3749 - accuracy: 0.3347 - 25s/epoch - 296ms/step\n",
      "Epoch 4/80\n",
      "84/84 - 25s - loss: 2.2303 - accuracy: 0.3675 - 25s/epoch - 296ms/step\n",
      "Epoch 5/80\n",
      "84/84 - 26s - loss: 2.1208 - accuracy: 0.3942 - 26s/epoch - 310ms/step\n",
      "Epoch 6/80\n",
      "84/84 - 31s - loss: 2.0312 - accuracy: 0.4148 - 31s/epoch - 371ms/step\n",
      "Epoch 7/80\n",
      "84/84 - 30s - loss: 1.9617 - accuracy: 0.4343 - 30s/epoch - 360ms/step\n",
      "Epoch 8/80\n",
      "84/84 - 29s - loss: 1.8961 - accuracy: 0.4519 - 29s/epoch - 349ms/step\n",
      "Epoch 9/80\n",
      "84/84 - 28s - loss: 1.8412 - accuracy: 0.4660 - 28s/epoch - 339ms/step\n",
      "Epoch 10/80\n",
      "84/84 - 28s - loss: 1.7884 - accuracy: 0.4797 - 28s/epoch - 337ms/step\n",
      "Epoch 11/80\n",
      "84/84 - 28s - loss: 1.7388 - accuracy: 0.4933 - 28s/epoch - 337ms/step\n",
      "Epoch 12/80\n",
      "84/84 - 29s - loss: 1.6979 - accuracy: 0.5055 - 29s/epoch - 341ms/step\n",
      "Epoch 13/80\n",
      "84/84 - 29s - loss: 1.6567 - accuracy: 0.5155 - 29s/epoch - 341ms/step\n",
      "Epoch 14/80\n",
      "84/84 - 28s - loss: 1.6179 - accuracy: 0.5248 - 28s/epoch - 339ms/step\n",
      "Epoch 15/80\n",
      "84/84 - 28s - loss: 1.5801 - accuracy: 0.5354 - 28s/epoch - 339ms/step\n",
      "Epoch 16/80\n",
      "84/84 - 29s - loss: 1.5431 - accuracy: 0.5448 - 29s/epoch - 342ms/step\n",
      "Epoch 17/80\n",
      "84/84 - 29s - loss: 1.5095 - accuracy: 0.5535 - 29s/epoch - 343ms/step\n",
      "Epoch 18/80\n",
      "84/84 - 28s - loss: 1.4774 - accuracy: 0.5627 - 28s/epoch - 336ms/step\n",
      "Epoch 19/80\n",
      "84/84 - 29s - loss: 1.4443 - accuracy: 0.5722 - 29s/epoch - 345ms/step\n",
      "Epoch 20/80\n",
      "84/84 - 29s - loss: 1.4142 - accuracy: 0.5795 - 29s/epoch - 340ms/step\n",
      "Epoch 21/80\n",
      "84/84 - 29s - loss: 1.3804 - accuracy: 0.5896 - 29s/epoch - 346ms/step\n",
      "Epoch 22/80\n",
      "84/84 - 29s - loss: 1.3523 - accuracy: 0.5971 - 29s/epoch - 345ms/step\n",
      "Epoch 23/80\n",
      "84/84 - 29s - loss: 1.3202 - accuracy: 0.6062 - 29s/epoch - 345ms/step\n",
      "Epoch 24/80\n",
      "84/84 - 29s - loss: 1.2900 - accuracy: 0.6153 - 29s/epoch - 340ms/step\n",
      "Epoch 25/80\n",
      "84/84 - 29s - loss: 1.2627 - accuracy: 0.6233 - 29s/epoch - 340ms/step\n",
      "Epoch 26/80\n",
      "84/84 - 29s - loss: 1.2284 - accuracy: 0.6324 - 29s/epoch - 342ms/step\n",
      "Epoch 27/80\n",
      "84/84 - 29s - loss: 1.2031 - accuracy: 0.6414 - 29s/epoch - 343ms/step\n",
      "Epoch 28/80\n",
      "84/84 - 29s - loss: 1.1711 - accuracy: 0.6510 - 29s/epoch - 344ms/step\n",
      "Epoch 29/80\n",
      "84/84 - 31s - loss: 1.1453 - accuracy: 0.6579 - 31s/epoch - 368ms/step\n",
      "Epoch 30/80\n",
      "84/84 - 29s - loss: 1.1111 - accuracy: 0.6682 - 29s/epoch - 344ms/step\n",
      "Epoch 31/80\n",
      "84/84 - 28s - loss: 1.0813 - accuracy: 0.6773 - 28s/epoch - 330ms/step\n",
      "Epoch 32/80\n",
      "84/84 - 27s - loss: 1.0534 - accuracy: 0.6864 - 27s/epoch - 327ms/step\n",
      "Epoch 33/80\n",
      "84/84 - 27s - loss: 1.0268 - accuracy: 0.6940 - 27s/epoch - 322ms/step\n",
      "Epoch 34/80\n",
      "84/84 - 27s - loss: 0.9944 - accuracy: 0.7040 - 27s/epoch - 326ms/step\n",
      "Epoch 35/80\n",
      "84/84 - 27s - loss: 0.9663 - accuracy: 0.7127 - 27s/epoch - 325ms/step\n",
      "Epoch 36/80\n",
      "84/84 - 27s - loss: 0.9377 - accuracy: 0.7208 - 27s/epoch - 326ms/step\n",
      "Epoch 37/80\n",
      "84/84 - 27s - loss: 0.9072 - accuracy: 0.7305 - 27s/epoch - 324ms/step\n",
      "Epoch 38/80\n",
      "84/84 - 28s - loss: 0.8856 - accuracy: 0.7375 - 28s/epoch - 338ms/step\n",
      "Epoch 39/80\n",
      "84/84 - 30s - loss: 0.8530 - accuracy: 0.7483 - 30s/epoch - 352ms/step\n",
      "Epoch 40/80\n",
      "84/84 - 27s - loss: 0.8286 - accuracy: 0.7546 - 27s/epoch - 325ms/step\n",
      "Epoch 41/80\n",
      "84/84 - 27s - loss: 0.7993 - accuracy: 0.7653 - 27s/epoch - 323ms/step\n",
      "Epoch 42/80\n",
      "84/84 - 28s - loss: 0.7668 - accuracy: 0.7745 - 28s/epoch - 328ms/step\n",
      "Epoch 43/80\n",
      "84/84 - 28s - loss: 0.7404 - accuracy: 0.7838 - 28s/epoch - 328ms/step\n",
      "Epoch 44/80\n",
      "84/84 - 27s - loss: 0.7201 - accuracy: 0.7888 - 27s/epoch - 323ms/step\n",
      "Epoch 45/80\n",
      "84/84 - 28s - loss: 0.6945 - accuracy: 0.7979 - 28s/epoch - 338ms/step\n",
      "Epoch 46/80\n",
      "84/84 - 29s - loss: 0.6639 - accuracy: 0.8081 - 29s/epoch - 347ms/step\n",
      "Epoch 47/80\n",
      "84/84 - 28s - loss: 0.6421 - accuracy: 0.8141 - 28s/epoch - 336ms/step\n",
      "Epoch 48/80\n",
      "84/84 - 28s - loss: 0.6271 - accuracy: 0.8180 - 28s/epoch - 333ms/step\n",
      "Epoch 49/80\n",
      "84/84 - 33s - loss: 0.5992 - accuracy: 0.8270 - 33s/epoch - 396ms/step\n",
      "Epoch 50/80\n",
      "84/84 - 31s - loss: 0.5844 - accuracy: 0.8314 - 31s/epoch - 364ms/step\n",
      "Epoch 51/80\n",
      "84/84 - 30s - loss: 0.5436 - accuracy: 0.8464 - 30s/epoch - 360ms/step\n",
      "Epoch 52/80\n",
      "84/84 - 28s - loss: 0.5253 - accuracy: 0.8525 - 28s/epoch - 336ms/step\n",
      "Epoch 53/80\n",
      "84/84 - 28s - loss: 0.5140 - accuracy: 0.8549 - 28s/epoch - 331ms/step\n",
      "Epoch 54/80\n",
      "84/84 - 28s - loss: 0.4903 - accuracy: 0.8636 - 28s/epoch - 329ms/step\n",
      "Epoch 55/80\n",
      "84/84 - 29s - loss: 0.4675 - accuracy: 0.8715 - 29s/epoch - 350ms/step\n",
      "Epoch 56/80\n",
      "84/84 - 30s - loss: 0.4532 - accuracy: 0.8751 - 30s/epoch - 354ms/step\n",
      "Epoch 57/80\n",
      "84/84 - 30s - loss: 0.4272 - accuracy: 0.8848 - 30s/epoch - 358ms/step\n",
      "Epoch 58/80\n",
      "84/84 - 30s - loss: 0.4110 - accuracy: 0.8899 - 30s/epoch - 353ms/step\n",
      "Epoch 59/80\n",
      "84/84 - 29s - loss: 0.3952 - accuracy: 0.8936 - 29s/epoch - 341ms/step\n",
      "Epoch 60/80\n",
      "84/84 - 29s - loss: 0.3757 - accuracy: 0.9012 - 29s/epoch - 341ms/step\n",
      "Epoch 61/80\n",
      "84/84 - 29s - loss: 0.3701 - accuracy: 0.9015 - 29s/epoch - 342ms/step\n",
      "Epoch 62/80\n",
      "84/84 - 31s - loss: 0.3443 - accuracy: 0.9115 - 31s/epoch - 367ms/step\n",
      "Epoch 63/80\n",
      "84/84 - 29s - loss: 0.3338 - accuracy: 0.9135 - 29s/epoch - 349ms/step\n",
      "Epoch 64/80\n",
      "84/84 - 31s - loss: 0.3286 - accuracy: 0.9144 - 31s/epoch - 372ms/step\n",
      "Epoch 65/80\n",
      "84/84 - 31s - loss: 0.3112 - accuracy: 0.9202 - 31s/epoch - 365ms/step\n",
      "Epoch 66/80\n",
      "84/84 - 30s - loss: 0.2997 - accuracy: 0.9239 - 30s/epoch - 362ms/step\n",
      "Epoch 67/80\n",
      "84/84 - 29s - loss: 0.2794 - accuracy: 0.9315 - 29s/epoch - 345ms/step\n",
      "Epoch 68/80\n",
      "84/84 - 29s - loss: 0.2699 - accuracy: 0.9336 - 29s/epoch - 348ms/step\n",
      "Epoch 69/80\n",
      "84/84 - 30s - loss: 0.2566 - accuracy: 0.9382 - 30s/epoch - 355ms/step\n",
      "Epoch 70/80\n",
      "84/84 - 29s - loss: 0.2488 - accuracy: 0.9401 - 29s/epoch - 349ms/step\n",
      "Epoch 71/80\n",
      "84/84 - 30s - loss: 0.2362 - accuracy: 0.9436 - 30s/epoch - 355ms/step\n",
      "Epoch 72/80\n",
      "84/84 - 28s - loss: 0.2229 - accuracy: 0.9476 - 28s/epoch - 333ms/step\n",
      "Epoch 73/80\n",
      "84/84 - 28s - loss: 0.2187 - accuracy: 0.9486 - 28s/epoch - 332ms/step\n",
      "Epoch 74/80\n",
      "84/84 - 28s - loss: 0.2186 - accuracy: 0.9472 - 28s/epoch - 332ms/step\n",
      "Epoch 75/80\n",
      "84/84 - 28s - loss: 0.2058 - accuracy: 0.9509 - 28s/epoch - 335ms/step\n",
      "Epoch 76/80\n",
      "84/84 - 28s - loss: 0.1998 - accuracy: 0.9522 - 28s/epoch - 339ms/step\n",
      "Epoch 77/80\n",
      "84/84 - 28s - loss: 0.1940 - accuracy: 0.9538 - 28s/epoch - 337ms/step\n",
      "Epoch 78/80\n",
      "84/84 - 29s - loss: 0.2031 - accuracy: 0.9497 - 29s/epoch - 350ms/step\n",
      "Epoch 79/80\n",
      "84/84 - 32s - loss: 0.1972 - accuracy: 0.9511 - 32s/epoch - 381ms/step\n",
      "Epoch 80/80\n",
      "84/84 - 29s - loss: 0.1965 - accuracy: 0.9509 - 29s/epoch - 344ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25455656530>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed\n",
    "\n",
    "hidden_units = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_units, input_shape=(None, train_X.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(hidden_units, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, epochs=80, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe7e335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, length):\n",
    "    # 문자에 대한 랜덤한 정수 생성\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "\n",
    "    # 랜덤한 정수로부터 맵핑되는 문자 생성\n",
    "    y_char = [index_to_char[ix[-1]]]\n",
    "    print(ix[-1],'번 문자',y_char[-1],'로 예측을 시작!')\n",
    "\n",
    "    # (1, length, 55) 크기의 X 생성. 즉, LSTM의 입력 시퀀스 생성\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "\n",
    "    for i in range(length):\n",
    "        # X[0][i][예측한 문자의 인덱스] = 1, 즉, 예측 문자를 다음 입력 시퀀스에 추가\n",
    "        X[0][i][ix[-1]] = 1\n",
    "        print(index_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(index_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c86e10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 번 문자 y 로 예측을 시작!\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 830ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "y the rabbit came up to the door, and tried to open it; but, if the wooldation por omenes, couldnt ge\n"
     ]
    }
   ],
   "source": [
    "result = sentence_generation(model, 100)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
